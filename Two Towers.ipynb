{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZUaPIAXxJYX"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "DYlaSDCI4M_3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4ddGekJxFss"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "VFnmmVVWxEXj"
      },
      "outputs": [],
      "source": [
        "# We should match the order with the input data\n",
        "user_feat       = {\n",
        "    # Feature     cardinality   embedding dimension\n",
        "    'User_ID':    [9238,        32],\n",
        "    'Timestamp':  [12,          4],   # Depends on the amount of 'bins' we use (e.g. 12 months)\n",
        "}\n",
        "\n",
        "# Input dimensions\n",
        "user_dim        = sum(np.array(list(user_feat.values()))[:, 1])\n",
        "item_dim        = 128\n",
        "aug_dim         = 32\n",
        "\n",
        "# Dimensions of the tower FFN\n",
        "hidden_dim      = 64\n",
        "embed_dim       = 32\n",
        "\n",
        "# lambda1 for loss_u & lambda2 for loss_V\n",
        "lambda1         = 1\n",
        "lambda2         = 1\n",
        "\n",
        "# Training\n",
        "num_epochs      = 10\n",
        "learning_rate   = 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMhdlqi8xQs4"
      },
      "source": [
        "### Dual Tower Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-ZAVfKUwfDD"
      },
      "outputs": [],
      "source": [
        "class Tower(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.normalize(self.feedforward(x), p = 2, dim = 1) # x-dims = B, F (batch, features)\n",
        "\n",
        "\n",
        "\n",
        "class DualAugmentedTwoTower(nn.Module):\n",
        "    def __init__(self, user_dim, item_dim, hidden_dim, embed_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # User feature embedding\n",
        "        self.user_embeddings = nn.ModuleList([\n",
        "            nn.Embedding(feat_cardinality, feat_embed_dim)\n",
        "            for feat_cardinality, feat_embed_dim in list(user_feat.values())\n",
        "        ])\n",
        "\n",
        "        # Tower initialisations\n",
        "        self.user_tower = Tower(user_dim + aug_dim, hidden_dim, embed_dim)\n",
        "        self.item_tower = Tower(item_dim + aug_dim, hidden_dim, embed_dim)\n",
        "        self.au = nn.Parameter(torch.randn(embed_dim))  # user augmented vector\n",
        "        self.av = nn.Parameter(torch.randn(embed_dim))  # item augmented vector\n",
        "\n",
        "    def forward(self, user_x, item_x, labels):\n",
        "\n",
        "        # user_x shape: (B, F) with indices for each categorical feature\n",
        "        user_embedding = [\n",
        "            embed(user_x[:, f]) for f, embed in enumerate(self.user_embeddings)\n",
        "        ]\n",
        "        user_x = torch.cat(user_embedding, dim = 1)  # shape (B, sum_embedding_dims)\n",
        "\n",
        "        # Expand augmented vectors to batch size\n",
        "        au_batch = self.au.expand(user_x.size(0), -1)  # shape (B, aug_dim)\n",
        "        av_batch = self.av.expand(item_x.size(0), -1)  # shape (B, aug_dim)\n",
        "\n",
        "        pu = self.user_tower(torch.cat((user_x, au_batch), dim=1))\n",
        "        pv = self.item_tower(torch.cat((item_x, av_batch), dim=1))\n",
        "\n",
        "        # Adaptive mimic mechanism (stop gradient for embeddings)\n",
        "        with torch.no_grad():\n",
        "            pu_detach = pu.detach()\n",
        "            pv_detach = pv.detach()\n",
        "\n",
        "        au = self.au + (pv_detach - self.au) * labels.unsqueeze(1) # au when label = 0, pv when label = 1\n",
        "        av = self.av + (pu_detach - self.av) * labels.unsqueeze(1) # av when label = 0, pu when label = 1\n",
        "\n",
        "        # Compute mimic losses (mean squared error)\n",
        "        loss_u = F.mse_loss(au, pv_detach) # 0 when label = 0\n",
        "        loss_v = F.mse_loss(av, pu_detach) # 0 when label = 0\n",
        "\n",
        "        # Final dot-product score\n",
        "        score = torch.sum(pu * pv, dim = 1)\n",
        "\n",
        "        return score, loss_u, loss_v\n",
        "\n",
        "    def loss(self, score, loss_u, loss_v, labels, lambda_u = 1, lambda_v = 1):\n",
        "        loss_p = F.binary_cross_entropy_with_logits(score, labels.float())\n",
        "        return loss_p + lambda_u * loss_u + lambda_v * loss_v\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6B26shgy0zO"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHKq5_65mAfa"
      },
      "outputs": [],
      "source": [
        "def train_model(model, dataloader, optimizer, device, num_epochs = 10, lambda_u = 1.0, lambda_v = 1.0):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0.0\n",
        "        for batch in dataloader:\n",
        "            user_x, item_x, labels = batch\n",
        "\n",
        "            # Move to device\n",
        "            user_x = user_x.to(device)\n",
        "            item_x = item_x.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            score, loss_u, loss_v = model(user_x, item_x, labels)\n",
        "\n",
        "            # Compute combined loss\n",
        "            loss = model.loss(score, loss_u, loss_v, labels, lambda_u, lambda_v)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item() * labels.size(0)\n",
        "\n",
        "        avg_loss = epoch_loss / len(dataloader.dataset)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}: Avg Loss = {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckLT9nW4ngU9"
      },
      "source": [
        "### Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmMWVgQTpTDl"
      },
      "source": [
        "TODO: implement dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVRlqD7dniDp"
      },
      "outputs": [],
      "source": [
        "model = DualAugmentedTwoTower(user_dim, item_dim, hidden_dim, embed_dim)\n",
        "optimiser = Adam(model.parameters(), lr = learning_rate)\n",
        "dataloader = None\n",
        "\n",
        "train_model(model, dataloader, optimiser)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
