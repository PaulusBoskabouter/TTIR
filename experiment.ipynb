{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ff46769",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86023d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.dataset_functions as df\n",
    "import utils.user_features as uf\n",
    "import utils.two_towers as ttn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from threading import Thread\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm as progress_bar\n",
    "\n",
    "\n",
    "\n",
    "data_dir = Path(\"Dataset\") / \"unprocessed\"\n",
    "data_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3677651",
   "metadata": {},
   "source": [
    "## 1. Download and write locally to CSV's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "349e9382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write files locally\n",
    "dataset_types = [\"likes\", \"listens\", \"dislikes\", \"unlikes\", \"undislikes\"]\n",
    "dataset = df.YambdaDataset('flat', '50m')\n",
    "for dt in dataset_types:\n",
    "    df.download_df(dataset=dataset, dataset_type=dt)\n",
    "\n",
    "\n",
    "if not (data_dir / \"embeddings.csv\").exists():\n",
    "    embeddings = dataset.audio_embeddings().to_pandas()\n",
    "    embeddings.to_csv(data_dir / \"embeddings.csv\", index=False)\n",
    "    del embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed24dc2",
   "metadata": {},
   "source": [
    "## 2. Load Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "501c8667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>uid</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>item_id</th>\n",
       "      <th>is_organic</th>\n",
       "      <th>played_ratio_pct</th>\n",
       "      <th>track_length_seconds</th>\n",
       "      <th>normalized_embed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39420</td>\n",
       "      <td>8326270</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>170</td>\n",
       "      <td>[-0.169998115, -0.0959603293, 0.0354052303, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>39420</td>\n",
       "      <td>1441281</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>105</td>\n",
       "      <td>[-0.11168661, -0.06717089, -0.01324262, -0.075...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>39625</td>\n",
       "      <td>286361</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>185</td>\n",
       "      <td>[-0.08362152, 0.01492759, 0.04177505, -0.07362...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>40110</td>\n",
       "      <td>732449</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>240</td>\n",
       "      <td>[-0.09272503, 0.00863106, 0.00500664, -0.07165...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>40360</td>\n",
       "      <td>3397170</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>130</td>\n",
       "      <td>[0.00739911, 0.02237171, -0.05895943, -0.04705...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9606652</th>\n",
       "      <td>9606652</td>\n",
       "      <td>4456</td>\n",
       "      <td>25757935</td>\n",
       "      <td>7156502</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>155</td>\n",
       "      <td>[-0.03618009, -0.01209565, -0.04070508, 0.0260...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9606653</th>\n",
       "      <td>9606653</td>\n",
       "      <td>4456</td>\n",
       "      <td>25758105</td>\n",
       "      <td>3117997</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>195</td>\n",
       "      <td>[-0.13640163, 0.09070778, -0.11365859, -0.0977...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9606654</th>\n",
       "      <td>9606654</td>\n",
       "      <td>4456</td>\n",
       "      <td>25777330</td>\n",
       "      <td>9224219</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>295</td>\n",
       "      <td>[-0.17506402, -0.07690737, -0.09771042, -0.106...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9606655</th>\n",
       "      <td>9606655</td>\n",
       "      <td>4456</td>\n",
       "      <td>25777360</td>\n",
       "      <td>8192914</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>155</td>\n",
       "      <td>[-0.0344837197, 0.0172222336, 0.00892607541, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9606656</th>\n",
       "      <td>9606656</td>\n",
       "      <td>4456</td>\n",
       "      <td>25777525</td>\n",
       "      <td>7227266</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>160</td>\n",
       "      <td>[-0.02479589, -0.08885409, 0.00972836, -0.0035...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9606657 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0   uid  timestamp  item_id  is_organic  played_ratio_pct  \\\n",
       "0                 0     0      39420  8326270           0               100   \n",
       "1                 1     0      39420  1441281           0               100   \n",
       "2                 2     0      39625   286361           0               100   \n",
       "3                 3     0      40110   732449           0               100   \n",
       "4                 4     0      40360  3397170           0                46   \n",
       "...             ...   ...        ...      ...         ...               ...   \n",
       "9606652     9606652  4456   25757935  7156502           0               100   \n",
       "9606653     9606653  4456   25758105  3117997           0                51   \n",
       "9606654     9606654  4456   25777330  9224219           1                 0   \n",
       "9606655     9606655  4456   25777360  8192914           1                15   \n",
       "9606656     9606656  4456   25777525  7227266           1               100   \n",
       "\n",
       "         track_length_seconds  \\\n",
       "0                         170   \n",
       "1                         105   \n",
       "2                         185   \n",
       "3                         240   \n",
       "4                         130   \n",
       "...                       ...   \n",
       "9606652                   155   \n",
       "9606653                   195   \n",
       "9606654                   295   \n",
       "9606655                   155   \n",
       "9606656                   160   \n",
       "\n",
       "                                          normalized_embed  \n",
       "0        [-0.169998115, -0.0959603293, 0.0354052303, -0...  \n",
       "1        [-0.11168661, -0.06717089, -0.01324262, -0.075...  \n",
       "2        [-0.08362152, 0.01492759, 0.04177505, -0.07362...  \n",
       "3        [-0.09272503, 0.00863106, 0.00500664, -0.07165...  \n",
       "4        [0.00739911, 0.02237171, -0.05895943, -0.04705...  \n",
       "...                                                    ...  \n",
       "9606652  [-0.03618009, -0.01209565, -0.04070508, 0.0260...  \n",
       "9606653  [-0.13640163, 0.09070778, -0.11365859, -0.0977...  \n",
       "9606654  [-0.17506402, -0.07690737, -0.09771042, -0.106...  \n",
       "9606655  [-0.0344837197, 0.0172222336, 0.00892607541, 0...  \n",
       "9606656  [-0.02479589, -0.08885409, 0.00972836, -0.0035...  \n",
       "\n",
       "[9606657 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# User like-dislike interactions\n",
    "likes = pd.read_csv(data_dir / \"likes.csv\", usecols=['uid', 'timestamp', 'item_id'])\n",
    "dislikes = pd.read_csv(data_dir / \"dislikes.csv\", usecols=['uid', 'timestamp', 'item_id'])\n",
    "unlikes = pd.read_csv(data_dir / \"unlikes.csv\", usecols=['uid', 'timestamp', 'item_id'])\n",
    "undislikes = pd.read_csv(data_dir / \"dislikes.csv\", usecols=['uid', 'timestamp', 'item_id'])\n",
    "\n",
    "\n",
    "\n",
    "# If not done before\n",
    "if (Path(\"Dataset\") / \"processed\" / \"merged.csv\").exists():\n",
    "    user_item_data = pd.read_csv(Path(\"Dataset\") / \"processed\" / \"merged.csv\", index_col=False)\n",
    "    user_item_data[\"normalized_embed\"] = user_item_data[\"normalized_embed\"].apply(df.parse_embedding)\n",
    "\n",
    "else:\n",
    "    # User listen interactions\n",
    "    listens = pd.read_csv(data_dir / \"listens.csv\", index_col=False)\n",
    "    listens.drop(columns=['is_organic'])\n",
    "\n",
    "    # due to computational limitations, we constrain our dataset to users to have between 500 and 5000 timestamps.\n",
    "    listens = listens.groupby('uid').filter(lambda x: 500 <= len(x) <= 5000)\n",
    "\n",
    "    # Embeddings\n",
    "    embeddings = pd.read_csv(data_dir/'embeddings.csv', usecols=['item_id', 'normalized_embed'])\n",
    "    embeddings[\"normalized_embed\"] = embeddings[\"normalized_embed\"].apply(df.parse_embedding)\n",
    "\n",
    "    # Merge the song embeddings and user listens dataset \n",
    "    user_item_data = pd.merge(listens, embeddings, on='item_id', how='inner')\n",
    "    \n",
    "    user_id_index = {}\n",
    "    for index, user in enumerate(user_item_data['uid'].unique()):\n",
    "        user_id_index[user] = index\n",
    "    user_item_data['uid'] = user_item_data['uid'].map(user_id_index)\n",
    "    #user_item_data.to_csv(Path(\"Dataset\") / \"processed\" / \"merged.csv\")\n",
    "\n",
    "    # save memory\n",
    "    del listens\n",
    "    del embeddings\n",
    "\n",
    "\n",
    "user_item_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc209db",
   "metadata": {},
   "source": [
    "## 3. Create and save user features\n",
    "We do this in train/val/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39be1e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 3128 users\n",
      "val  : 883 users\n",
      "test : 446 users\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81a9a600be54335ad5377f6c176c627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/781 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e33992a2744d4ab2ff443f1b5fb29d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/883 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b42cf886a3ec418a944ac8d837168bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/781 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c543f24b2a274e45a35684291affa48b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f73176cac9b42b6bfc87b59e004b503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/781 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c9de821f6a24a7eb9b671040288415f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/446 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "users = user_item_data['uid'].unique()\n",
    "\n",
    "__train_val_set, test_set = train_test_split(\n",
    "    users,\n",
    "    test_size=0.10,   # 10 % is test data\n",
    "    random_state=42,        # reproducible shuffling\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "train_set, val_set = train_test_split(\n",
    "    __train_val_set,\n",
    "    test_size=0.22,   # ~20% validation\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "print(\"train:\", len(train_set), \"users\")\n",
    "print(\"val  :\", len(val_set), \"users\")\n",
    "print(\"test :\", len(test_set), \"users\")\n",
    "\n",
    "# poorly designed split, but I'm okay with losing a few users (we have more than enough anyway).\n",
    "# Added + 1 in the indices to ensure no duplicates (still horrible method though).\n",
    "train_split = len(train_set)//4 \n",
    "\n",
    "# Multithread it to make it somewhat time managable\n",
    "t1 = Thread(target=uf.extract_and_save_features, args=(train_set[0:train_split], user_item_data, likes, unlikes, dislikes, undislikes, 'train'))\n",
    "t2 = Thread(target=uf.extract_and_save_features, args=(train_set[train_split+1:2*train_split], user_item_data, likes, unlikes, dislikes, undislikes, 'train'))\n",
    "t3 = Thread(target=uf.extract_and_save_features, args=(train_set[2*train_split+1:3*train_split], user_item_data, likes, unlikes, dislikes, undislikes, 'train'))\n",
    "t4 = Thread(target=uf.extract_and_save_features, args=(train_set[3*train_split+1:4*train_split], user_item_data, likes, unlikes, dislikes, undislikes, 'train'))\n",
    "t5 = Thread(target=uf.extract_and_save_features, args=(val_set, user_item_data, likes, unlikes, dislikes, undislikes, 'val'))\n",
    "t6 = Thread(target=uf.extract_and_save_features, args=(test_set, user_item_data, likes, unlikes, dislikes, undislikes, 'test'))\n",
    "\n",
    "t1.start()\n",
    "t2.start()\n",
    "t3.start()\n",
    "t4.start()\n",
    "t5.start()\n",
    "t6.start()\n",
    "\n",
    "\n",
    "t1.join()\n",
    "t2.join()\n",
    "t3.join()\n",
    "t4.join()\n",
    "t5.join()\n",
    "t6.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b18490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# free up memory, we don't need this anymore\n",
    "del user_item_data \n",
    "del likes\n",
    "del unlikes\n",
    "del dislikes\n",
    "del undislikes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9888d341",
   "metadata": {},
   "source": [
    "### 3.1: merge the seperate user files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6c6dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_type in ['train', 'val', 'test']:\n",
    "    files = Path(\"Dataset\") / \"processed\" / data_type\n",
    "    user_feats = []\n",
    "    user_ids = []\n",
    "    song_embeds = []\n",
    "    labels = []\n",
    "\n",
    "    for file in progress_bar(list(files.glob(\"*.pt\")), desc=f\"loading and merging {data_type} files\"):\n",
    "        loaded = torch.load(file, map_location=\"cpu\")\n",
    "        user_feats.append(loaded[\"user_feats\"])\n",
    "        user_ids.append(loaded[\"user_ids\"])\n",
    "        song_embeds.append(loaded[\"song_embeds\"])\n",
    "        labels.append(loaded[\"labels\"])\n",
    "\n",
    "        del loaded\n",
    "    \n",
    "    user_feats   = torch.cat(user_feats, dim=0)\n",
    "    song_embeds  = torch.cat(song_embeds, dim=0)\n",
    "    labels       = torch.cat(labels, dim=0)\n",
    "\n",
    "    uf.save_tensor_dataset(file_name=data_type, user_feats=user_feats, user_ids=user_ids, song_embeds=song_embeds, labels=labels, file_loc=Path(\"Dataset\"))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6194c838",
   "metadata": {},
   "source": [
    "## 4. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d528b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input dimensions\n",
    "user_dim        = 6\n",
    "item_dim        = 128\n",
    "aug_dim         = 32\n",
    "\n",
    "# Dimensions of the tower FFN\n",
    "hidden_dim      = 64\n",
    "embed_dim       = 32\n",
    "\n",
    "# lambda1 for loss_u & lambda2 for loss_V\n",
    "lambda1         = 1\n",
    "lambda2         = 1\n",
    "\n",
    "# Training\n",
    "num_epochs      = 10\n",
    "learning_rate   = 0.001\n",
    "batch_size      = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e949b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = uf.load_tensor_dataloader(\"train\", Path(\"Dataset\"), batch_size)\n",
    "val_set = uf.load_tensor_dataloader(\"val\", Path(\"Dataset\"), batch_size)\n",
    "test_set = uf.load_tensor_dataloader(\"test\", Path(\"Dataset\"), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3930b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With rf_signal we determine on what predetermined label (reinforcement signal) we want to train on.\n",
    "# Note: This should be an int between the values 0 and 3.\n",
    "# Cheat sheet: [interactions_label, multiple_listens_label, pct_100_label, pct_80_label]\n",
    "\n",
    "model = ttn.DualAugmentedTwoTower(\"interactions_model\", user_dim, item_dim, aug_dim, hidden_dim, embed_dim, rf_signal=0)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "ttn.train_model(model, train_set, val_set, optimiser)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
